{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "surprising-exhaust",
   "metadata": {},
   "source": [
    "## Setting Dataset\n",
    "In this notebook we will setup our dataset, we will read the dataset and apply all the preprocessing and then save that for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-plane",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "For reading images and resizing them we will need `open cv` library, and we will also need `numpy` library. After reading and resizing the images we will convert the image array into numpy array so then it can be used in Neural Networks. We will use `pickle` library to store our numpy array of images so then we can use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "exceptional-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-minneapolis",
   "metadata": {},
   "source": [
    "### Initializing variables\n",
    "We will need some global variables which will be used in the notebook.\n",
    "\n",
    "- Training dataset path.\n",
    "- Testing dataset path.\n",
    "- Total Classes in the dataset.\n",
    "- Size of the image, height and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "posted-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(os.getcwd(), 'DataSet')\n",
    "test_path = os.path.join(os.getcwd(), 'test')\n",
    "\n",
    "# List of classes\n",
    "categories = os.listdir(train_path)\n",
    "\n",
    "# Image size width and height\n",
    "IMG_SIZE = 125"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-mention",
   "metadata": {},
   "source": [
    "### Reading Dataset\n",
    "This function will be used to read the images, we will need to pass the dataset path, list of classes and the number to read maximum number of images for each class. This function will first read the image by using **cv2.imread()** function and then after reading the image this function will resize the image by using **cv2.resize()** function, this function will need the image height and width, we will pass **IMG_SIZE** for both height and width. For our project we will convert the images into **Gray Scale**, this will help to make the model computation faster as compared to **RGB image**. We will convert the image into gray scale while reading the image. In the image reading function **imread()** we will pass the second argument as **cv2.IMREAD_GRAYSCALE**, this will convert the image into gray scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "young-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(categories, data_path, max_images):\n",
    "    data = []\n",
    "\n",
    "    for category in categories:\n",
    "        path = os.path.join(data_path, category)\n",
    "        class_num = categories.index(category)\n",
    "        for img in os.listdir(path)[:max_images]:\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "                resized_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "                data.append([resized_array, class_num])\n",
    "            except Exception:\n",
    "                print(\"Exception\")\n",
    "                pass\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-ethernet",
   "metadata": {},
   "source": [
    "### Setting Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "occupational-printer",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = read_data(categories, train_path, 210)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-mailing",
   "metadata": {},
   "source": [
    "Shuffle the training data, this will help the model to learn better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aquatic-tunisia",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-elite",
   "metadata": {},
   "source": [
    "Creating X_train and y_train variables to store training features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "centered-happening",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for feature, label in training_data:\n",
    "    X_train.append(feature)\n",
    "    y_train.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-demonstration",
   "metadata": {},
   "source": [
    "Now in this section below we will reshape the training dataset, again we are doing this so this will make the computation faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "enhanced-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train).reshape(-1, IMG_SIZE, IMG_SIZE, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-indiana",
   "metadata": {},
   "source": [
    "#### Spliting Training and Validation\n",
    "Now we will split training and validation sets, the validation set is important for training the neural network as it will help us to prevent overfiting and underfiting problems and by the help of validation set it will give us better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "steady-minority",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-summer",
   "metadata": {},
   "source": [
    "## Normalizing dataset\n",
    "Now we will normalize the dataset, we have read the images in gray scale mode and now we know that each image is consist of array with numbers 0 to 255, so in this case we will normalize the dataset by dividing each image matrix with 255. This normalization will help us achieve better result and the computational loss of the model will also going to be low with the help of normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "intense-ethics",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')/255\n",
    "X_valid = X_valid.astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-wrong",
   "metadata": {},
   "source": [
    "## Setting Testing Dataset\n",
    "Now after setting up training dataset we will going to setup testing dataset. The steps for reading and setting up testing dataset is same as training dataset.\n",
    "\n",
    "In testing we will use 10 images for each class of the disease. The model will gonna test on total 50 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "returning-leave",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = read_data(categories, test_path, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "local-africa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for feature, label in testing_data:\n",
    "    X_test.append(feature)\n",
    "    y_test.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "selective-violin",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test).reshape(-1, IMG_SIZE, IMG_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "occasional-tulsa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-character",
   "metadata": {},
   "source": [
    "## Saving Testing, Training and Validation datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "civic-discipline",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/X_train.pickle', 'wb') as f:\n",
    "    pickle.dump(X_train, f)\n",
    "    \n",
    "with open('Data/X_valid.pickle', 'wb') as f:\n",
    "    pickle.dump(X_valid, f)\n",
    "    \n",
    "with open('Data/X_test.pickle', 'wb') as f:\n",
    "    pickle.dump(X_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "warming-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/y_train.pickle', 'wb') as f:\n",
    "    pickle.dump(y_train, f)\n",
    "    \n",
    "with open('Data/y_valid.pickle', 'wb') as f:\n",
    "    pickle.dump(y_valid, f)\n",
    "    \n",
    "with open('Data/y_test.pickle', 'wb') as f:\n",
    "    pickle.dump(y_test, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
